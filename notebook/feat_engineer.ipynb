{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ROOT_PATH = '/sources/dataset'\n",
    "VER = 'ver0'\n",
    "SAVE_PATH = os.path.join(ROOT_PATH, 'features', VER)\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "description = \\\n",
    "    \"\"\" ver 0. \\n \n",
    "        얼굴 랜드마크:\n",
    "        LIPSOUT_LM = [0, 267, 269, 270, 409, 287, 375, 321, 405, 314, 17, 84, 181, 91, 146, 57, 185, 40, 39, 37]\n",
    "        LIPSIN_LM = [13, 312, 311, 310, 415, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95, 78, 191, 80, 81, 82]\n",
    "\n",
    "        포즈 랜드마크: \n",
    "        PNOSE_LM = [0]\n",
    "        PFACE_LM = [8, 6, 5, 4, 1,2,3,7]\n",
    "        BODY_LM = [11, 12, 24 ,23]\n",
    "        ARM_LM = [14, 16, 22, 20, 18, 13, 15, 21, 19, 17]\n",
    "\n",
    "        손 랜드마크: 전부 사용\n",
    "\n",
    "        최종 데이터 형태: [1, 3150]\n",
    "\n",
    "        데이터 생성 파이프라인 요약\n",
    "        1. 랜드마크 선택: [N, 543, 3] -> [N, 105, 3]\n",
    "        2. 프레임 interpolation: N % SEGMENT == 0, SEGMENT=5     \n",
    "        3. SEGMENT 별로 프레임 축에 대해서 mean [SEGMENT, 105, 3] , std 계산: [SEGMENT, 105, 3] -> [2*SEGMENT, 105, 3] (mean*5, std*5)\n",
    "        3.1 mean, std 계산 시에 NaN -> 0 하여 계산에 미포함 \n",
    "        4. Flatten: [1, 3150]\n",
    "    \"\"\"\n",
    "\n",
    "with open(os.path.join(ROOT_PATH, 'features', VER, 'description.txt'), 'w') as f:\n",
    "    f.write(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.ndimage import zoom\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "SEGMENTS = 5\n",
    "LEFT_HAND_OFFSET = 468\n",
    "POSE_OFFSET = LEFT_HAND_OFFSET+21\n",
    "RIGHT_HAND_OFFSET = POSE_OFFSET+33\n",
    "\n",
    "LIPSOUT_LM = [0, 267, 269, 270, 409, 287, 375, 321, 405, 314, 17, 84, 181, 91, 146, 57, 185, 40, 39, 37]\n",
    "LIPSIN_LM = [13, 312, 311, 310, 415, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95, 78, 191, 80, 81, 82]\n",
    "\n",
    "PNOSE_LM = [0]\n",
    "PFACE_LM = [8, 6, 5, 4, 1,2,3,7]\n",
    "BODY_LM = [11, 12, 24 ,23]\n",
    "ARM_LM = [14, 16, 22, 20, 18, 13, 15, 21, 19, 17]\n",
    "\n",
    "lip_landmarks = LIPSIN_LM + LIPSOUT_LM\n",
    "pose_landmarks = PNOSE_LM + PFACE_LM + BODY_LM + ARM_LM\n",
    "left_hand_landmarks = list(range(LEFT_HAND_OFFSET, LEFT_HAND_OFFSET+21))\n",
    "right_hand_landmarks = list(range(RIGHT_HAND_OFFSET, RIGHT_HAND_OFFSET+21))\n",
    "\n",
    "point_landmarks =  [item for sublist in [lip_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks] for item in sublist]\n",
    "LANDMARKS = len(point_landmarks)\n",
    "print(LANDMARKS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_nan_mean(x, axis=1):\n",
    "    nan_mask = torch.isnan(x)\n",
    "    zero_mask = torch.zeros_like(x)\n",
    "    ones_mask = torch.ones_like(x)\n",
    "    \n",
    "    # Replace NaN values with zeros\n",
    "    x = torch.where(nan_mask, zero_mask, x)\n",
    "\n",
    "    # Compute the sum of non-NaN values along the specified axis\n",
    "    sum_values = torch.sum(x, dim=axis)\n",
    "    count_values = torch.sum(torch.where(nan_mask, zero_mask, ones_mask), dim=axis)\n",
    "    \n",
    "    # Compute the mean\n",
    "    mean_values = sum_values / count_values\n",
    "    \n",
    "    return mean_values\n",
    "\n",
    "def torch_nan_std(x, axis=1):\n",
    "    mean_values = torch_nan_mean(x, axis=axis)\n",
    "\n",
    "    d = x - mean_values.unsqueeze(1)\n",
    "    return torch.sqrt(torch_nan_mean(d * d, axis=axis))\n",
    "\n",
    "def fill_nan_zero(x):\n",
    "    nan_mask = torch.isnan(x)\n",
    "    zero_mask = torch.zeros_like(x)\n",
    "\n",
    "    # Replace NaN values with zeros\n",
    "    x = torch.where(nan_mask, zero_mask, x)\n",
    "    return x\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Generation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureGen():\n",
    "    def __init__(self, segments: int=5):\n",
    "        LEFT_HAND_OFFSET = 468\n",
    "        POSE_OFFSET = LEFT_HAND_OFFSET+21\n",
    "        RIGHT_HAND_OFFSET = POSE_OFFSET+33\n",
    "\n",
    "        LIPSOUT_LM = [0, 267, 269, 270, 409, 287, 375, 321, 405, 314, 17, 84, 181, 91, 146, 57, 185, 40, 39, 37]\n",
    "        LIPSIN_LM = [13, 312, 311, 310, 415, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95, 78, 191, 80, 81, 82]\n",
    "\n",
    "        PNOSE_LM = [0]\n",
    "        PFACE_LM = [8, 6, 5, 4, 1,2,3,7]\n",
    "        BODY_LM = [11, 12, 24 ,23]\n",
    "        ARM_LM = [14, 16, 22, 20, 18, 13, 15, 21, 19, 17]\n",
    "\n",
    "        lip_landmarks = LIPSIN_LM + LIPSOUT_LM\n",
    "        pose_landmarks = PNOSE_LM + PFACE_LM + BODY_LM + ARM_LM\n",
    "        left_hand_landmarks = list(range(LEFT_HAND_OFFSET, LEFT_HAND_OFFSET+21))\n",
    "        right_hand_landmarks = list(range(RIGHT_HAND_OFFSET, RIGHT_HAND_OFFSET+21))\n",
    "\n",
    "        self.point_landmarks =  [item for sublist in [lip_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks] for item in sublist]\n",
    "        self.segments = segments\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = np.take(x, self.point_landmarks, axis=1)    # [N, 105, 3]\n",
    "        n_frame, num_landmark, num_coord = x.shape[0], x.shape[1], x.shape[2]\n",
    "        new_n_frame = n_frame + (self.segments - (n_frame % self.segments))\n",
    "        x = zoom(x, (new_n_frame, num_landmark, num_coord) / np.array(x.shape), order=1)     # [N', num_ladmark, 3]\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        frame_per_seg = x.shape[0] // self.segments\n",
    "        x = x.view(-1, frame_per_seg, 105, 3)    # [segments, frame_per_seg, num_landmark, 3]\n",
    "\n",
    "        x_mean = fill_nan_zero(torch_nan_mean(x))   # [segments, num_landmark, 3]\n",
    "        x_std = fill_nan_zero(torch_nan_std(x))  # [segments, num_landmark, 3]\n",
    "\n",
    "        feat = torch.cat([x_mean, x_std], axis=0)   # [2*segments, num_landmark, 3]\n",
    "        feat = feat.view(1, -1) # [1, 2*segments * num_landmark * 3]\n",
    "\n",
    "        return feat\n",
    "    \n",
    "feat_converter = FeatureGen()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert raw data and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def convert_row(row, right_handed=True):\n",
    "    x = load_relevant_data_subset(os.path.join(ROOT_PATH, row.path))\n",
    "    x = feat_converter(x).cpu().numpy()\n",
    "    return x, row.label\n",
    "\n",
    "def convert_and_save_data():\n",
    "    df = pd.read_csv(os.path.join(ROOT_PATH, 'train.csv'))\n",
    "    label_map = json.load(open(os.path.join(ROOT_PATH, 'sign_to_prediction_index_map.json')))\n",
    "    df['label'] = df['sign'].map(label_map)\n",
    "\n",
    "    total = df.shape[0]\n",
    "\n",
    "    npdata = np.zeros((total, 2*SEGMENTS*LANDMARKS*3))\n",
    "    nplabels = np.zeros(total)\n",
    "\n",
    "    for i, row in tqdm(enumerate(df.itertuples()), total=total):\n",
    "        (x, y) = convert_row(row)\n",
    "        npdata[i, :] = x\n",
    "        nplabels[i] = y\n",
    "\n",
    "        if i == total - 1:\n",
    "            break\n",
    "    \n",
    "    np.save(os.path.join(SAVE_PATH, 'feature_data.npy'), npdata)\n",
    "    np.save(os.path.join(SAVE_PATH, 'feature_labels.npy'), nplabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 94476/94477 [09:28<00:00, 166.10it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_and_save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94477, 3150) (94477,)\n",
      "(3150,) [ 0.50394118  0.39283019 -0.01993244 ...  0.          0.\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "X = np.load(os.path.join(SAVE_PATH, 'feature_data.npy'))\n",
    "y = np.load(os.path.join(SAVE_PATH, 'feature_labels.npy'))\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "print(X[0, :].shape, X[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
